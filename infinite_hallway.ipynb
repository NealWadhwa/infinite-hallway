{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Infinite Hallway Generator</h1>\n",
    "\n",
    "Generate infinite videos of hallways using stable diffusion and monocular depth estimation.\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/NealWadhwa/infinite-hallway/blob/main/infinite_hallway.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install matplotlib diffusers torch torchvision torchgeometry\n",
    "%pip install timm transformers ipywidgets accelerate opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchgeometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to hugging face\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MiDaS model for depth estimation.\n",
    "model_type = \"DPT_Large\"\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n",
    "midas.to(device)\n",
    "midas.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load inpainting stable diffusion model\n",
    "from diffusers import StableDiffusionInpaintPipeline, DiffusionPipeline, DPMSolverMultistepScheduler\n",
    "\n",
    "pipe_inpaint = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2-inpainting\",\n",
    "    torch_dtype=torch.float16,\n",
    "    revision=\"fp16\")\n",
    "pipe_inpaint.scheduler = DPMSolverMultistepScheduler.from_config(\n",
    "    pipe_inpaint.scheduler.config)\n",
    "pipe_inpaint = pipe_inpaint.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def zoom_mpi(image,\n",
    "             inverse_depth,\n",
    "             focal_length,\n",
    "             rotation,\n",
    "             translation,\n",
    "             a=0.0,\n",
    "             b=1.0,\n",
    "             mpi_layers=16,\n",
    "             device=\"cuda:0\"):\n",
    "    rotation = rotation.to(device)\n",
    "    translation = translation.to(device)\n",
    "\n",
    "    # Break the image into a MPI based on depth.\n",
    "    max_depth = inverse_depth.max()\n",
    "    min_depth = inverse_depth.min()\n",
    "    normalized_depth = (inverse_depth - min_depth) / (max_depth - min_depth)\n",
    "\n",
    "    # Hard cutoffs in the MPI\n",
    "    mpi = torch.floor(normalized_depth * mpi_layers).to(torch.int32)\n",
    "    depth = torch.arange(mpi_layers + 1).to(device) / mpi_layers\n",
    "    depth = depth * (max_depth - min_depth) + min_depth\n",
    "    masks = []\n",
    "    layers = []\n",
    "    _, _, height, width = image.shape\n",
    "    dsize = (height, width)\n",
    "    out_image = torch.zeros_like(image)\n",
    "    out_mask = torch.zeros_like(inverse_depth)\n",
    "    for i in range(mpi_layers + 1):\n",
    "        mask = (mpi == i).to(torch.float)\n",
    "\n",
    "        intrinsic1 = torch.FloatTensor(\n",
    "            [[1.0 / focal_length, 0, -height / 2 / focal_length],\n",
    "             [0, 1.0 / focal_length, -width / 2 / focal_length], [0, 0, 1.0],\n",
    "             [0.0, 0.0, depth[i]]]).to(device)\n",
    "        intrinsic2 = torch.FloatTensor([[focal_length, 0, height / 2],\n",
    "                                        [0, focal_length, width / 2],\n",
    "                                        [0, 0, 1.0]]).to(device)\n",
    "        rt = torch.cat([rotation, translation], dim=1)\n",
    "\n",
    "        ptrans = torch.matmul(rt, intrinsic1)\n",
    "        perspective_transform = torch.matmul(intrinsic2, ptrans)\n",
    "        warped_mask = torchgeometry.warp_perspective(mask[None, :, :, :],\n",
    "                                                     perspective_transform,\n",
    "                                                     dsize=dsize)\n",
    "        warped_image = torchgeometry.warp_perspective(image,\n",
    "                                                      perspective_transform,\n",
    "                                                      dsize=dsize)\n",
    "\n",
    "        warped_image = torch.clip(warped_image, 0, 1)\n",
    "        warped_mask = torch.clip(warped_mask, 0, 1)\n",
    "        warped_layer = warped_image * warped_mask\n",
    "\n",
    "        out_image = warped_layer + out_mask * out_image * (1 - warped_mask[0])\n",
    "        out_mask = warped_mask[0] + out_mask * (1 - warped_mask[0])\n",
    "        out_image = out_image / (1e-16 + out_mask)\n",
    "\n",
    "    binary_mask = (out_mask == 0).to(torch.float32)\n",
    "    out_image = out_image * (1 - binary_mask)\n",
    "\n",
    "    return out_image, (out_mask == 0).to(torch.float32)\n",
    "\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "\n",
    "def to_uint8(tensor):\n",
    "    return (to_numpy(tensor) * 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_frame(image,\n",
    "                        prompt,\n",
    "                        focal_length,\n",
    "                        rotation,\n",
    "                        translation,\n",
    "                        negative_prompt=\"\",\n",
    "                        a=0.0,\n",
    "                        b=1.0,\n",
    "                        mpi_layers=16,\n",
    "                        device=\"cuda:0\"):\n",
    "    to_tensor = torchvision.transforms.ToTensor()\n",
    "    # Compute the depth map of the image\n",
    "    image = image.to(device)\n",
    "    image = image[None, :, :, :]\n",
    "    with torch.inference_mode():\n",
    "        depth = midas(image)\n",
    "\n",
    "    warped, mask = zoom_mpi(image, depth, focal_length, rotation, translation,\n",
    "                            a, b, mpi_layers, device)\n",
    "\n",
    "    warped_scaled = torch.clip(2 * warped - 1, -1, 1)\n",
    "\n",
    "    inpainted = pipe_inpaint(prompt=prompt,\n",
    "                             negative_prompt=negative_prompt,\n",
    "                             image=warped_scaled,\n",
    "                             mask_image=mask).images[0]\n",
    "    inpainted = to_tensor(inpainted)[None, :, :, :]\n",
    "    inpainted = inpainted.to(device)\n",
    "    inpainted = warped * (1 - mask) + inpainted * mask\n",
    "    return inpainted[0, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A large cat in a hallway with snakes and vines in an hallway in a steampunk aztec temple made of gears\"\n",
    "negative_prompt = \"blurry, bad art, blurred, text, watermark\"\n",
    "\n",
    "NUM_FRAMES = 100\n",
    "\n",
    "prompts = [prompt] * NUM_FRAMES\n",
    "\n",
    "to_tensor = torchvision.transforms.ToTensor()\n",
    "\n",
    "dummy_image = torch.ones((1, 3, 512, 512), dtype=torch.float32).to(\"cuda:0\")\n",
    "mask = torch.ones((1, 1, 512, 512), dtype=torch.float32).to(\"cuda:0\")\n",
    "image = pipe_inpaint(prompt=prompt,\n",
    "                     negative_prompt=negative_prompt,\n",
    "                     image=dummy_image,\n",
    "                     mask_image=mask).images[0]\n",
    "image = to_tensor(image)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('/tmp/output.avi', fourcc, 15.0, (512, 512))\n",
    "out.write(to_uint8(image))\n",
    "\n",
    "for prompt in prompts:\n",
    "    image = generate_next_frame(image,\n",
    "                                prompt,\n",
    "                                negative_prompt=negative_prompt,\n",
    "                                focal_length=30,\n",
    "                                rotation=torch.eye(3),\n",
    "                                translation=torch.FloatTensor([[0.0], [0.0],\n",
    "                                                               [0.003]]))\n",
    "    image_np = to_uint8(image)\n",
    "    Image.fromarray(image_np).save(\"/tmp/spice.png\")\n",
    "    out.write(image_np)\n",
    "\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8fff095a99c04e73b35db3bc47f5c15b2a5e16fd942f005a37d048bb4acdea5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
